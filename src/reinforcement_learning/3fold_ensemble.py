import json
import numpy as np
import random
import torch
import torch.nn as nn
import os
from collections import deque

# =========================================================
# [Part 1] í•˜ìœ„ ì—ì´ì „íŠ¸ ë° IQN ë„¤íŠ¸ì›Œí¬ (ê·¸ëŒ€ë¡œ ìœ ì§€)
# =========================================================
class AdvancedAgent: 
    def __init__(self, n_arms=2):
        self.n_arms = n_arms; self.q_values = np.ones(n_arms) * 0.5; self.tau, self.alpha = 0.1, 0.1
    def select_arm(self):
        pref = self.q_values - np.max(self.q_values)
        probs = np.exp(pref/self.tau) / np.sum(np.exp(pref/self.tau))
        return np.random.choice(self.n_arms, p=probs)
    def update(self, arm, reward): self.q_values[arm] += self.alpha * (reward - self.q_values[arm])

class HeonAgent: 
    def __init__(self, n_arms=2):
        self.n_arms = n_arms; self.win = 65; self.c = 0.8; self.hist = []
    def select_arm(self):
        if len(self.hist) < self.n_arms: return len(self.hist) % self.n_arms
        cw = self.hist[-self.win:]; cnts = np.zeros(self.n_arms); vals = np.zeros(self.n_arms)
        for a, r in cw: cnts[a]+=1; vals[a]+=r
        ucb = [vals[a]/cnts[a] + self.c*np.sqrt(np.log(len(cw))/cnts[a]) if cnts[a]>0 else 1e5 for a in range(self.n_arms)]
        return np.argmax(ucb)
    def update(self, a, r): self.hist.append((a, r))

class StockAgent: 
    def __init__(self, n_arms=2):
        self.n_arms = n_arms; self.last_a = None; self.last_r = None; self.loss_cnt = 0
    def select_arm(self):
        if self.last_a is None: return random.randint(0, self.n_arms-1)
        if self.last_r == 1: return self.last_a
        return 1-self.last_a if self.loss_cnt >= 2 else self.last_a
    def update(self, a, r):
        self.last_a = a; self.last_r = r; self.loss_cnt = self.loss_cnt + 1 if r == 0 else 0

class IQN_Network(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=128, n_quantiles=32):
        super(IQN_Network, self).__init__()
        self.input_dim = input_dim; self.output_dim = output_dim; self.n_quantiles = n_quantiles
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.feature_layer = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU())
        self.phi = nn.Sequential(nn.Linear(64, hidden_dim), nn.ReLU())
        self.fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, output_dim))
        self.pis = torch.FloatTensor([np.pi * i for i in range(1, 65)]).view(1, 1, 64).to(self.device)

    def forward(self, state, num_quantiles=None):
        if num_quantiles is None: num_quantiles = self.n_quantiles
        batch_size = state.shape[0]
        x = self.feature_layer(state)
        tau = torch.rand(batch_size, num_quantiles).to(self.device)
        tau_embed = torch.cos(tau.unsqueeze(-1) * self.pis)
        tau_embed = self.phi(tau_embed)
        x = x.unsqueeze(1)
        z = x * tau_embed
        quantiles = self.fc(z)
        return quantiles, tau

# =========================================================
# [Part 2] 3-Fold IQN ì•™ìƒë¸” ì—ì´ì „íŠ¸ (ìµœì í™”ë¨)
# =========================================================
class IQN3FoldEnsembleAgent:
    def __init__(self, n_arms=2):
        self.n_arms = n_arms
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.action_dim = 3
        self.state_dim = 12
        
        # 1. ëª¨ë¸ ë¡œë”© (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        self.models = []
        model_files = ['iqn_fold_0.pth', 'iqn_fold_1.pth', 'iqn_fold_2.pth']
        
        print(f"ğŸ”„ Loading Models on {self.device}...")
        for f in model_files:
            if os.path.exists(f):
                model = IQN_Network(self.state_dim, self.action_dim).to(self.device)
                # weights_only=True ì˜µì…˜ì„ ì“°ë©´ ê²½ê³ ê°€ ì‚¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‚˜, í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
                model.load_state_dict(torch.load(f, map_location=self.device))
                model.eval()
                self.models.append(model)
                print(f"  âœ… Loaded: {f}")
            else:
                print(f"  âš ï¸ Missing: {f}")
        
        # 2. ë‚´ë¶€ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.reset_episode()

    def reset_episode(self):
        """ì—í”¼ì†Œë“œë§ˆë‹¤ í˜¸ì¶œ: í•˜ìœ„ ì—ì´ì „íŠ¸ì™€ ê¸°ì–µ ì´ˆê¸°í™”"""
        self.sub_agents = [StockAgent(self.n_arms), HeonAgent(self.n_arms), AdvancedAgent(self.n_arms)]
        self.history = deque(maxlen=6)
        for _ in range(6): self.history.append((0,0))
        self.last_decisions = []
        self.last_action = 0

    def get_state(self):
        flat = []
        for a, r in self.history: flat.extend([a, r])
        return torch.FloatTensor(flat).to(self.device)

    def select_arm(self):
        # í•˜ìœ„ ì—ì´ì „íŠ¸ ì„ íƒ
        act_stock = self.sub_agents[0].select_arm()
        act_heon = self.sub_agents[1].select_arm()
        act_adv = self.sub_agents[2].select_arm()
        self.last_decisions = [act_stock, act_heon, act_adv]
        
        # ì•™ìƒë¸” ì¶”ë¡  (Soft Voting)
        if not self.models:
            meta_action = random.randint(0, 2)
        else:
            state = self.get_state()
            with torch.no_grad():
                state_tensor = state.unsqueeze(0)
                avg_q = torch.zeros(1, self.action_dim).to(self.device)
                
                for model in self.models:
                    quantiles, _ = model(state_tensor)
                    q_values = quantiles.mean(dim=1)
                    avg_q += q_values
                
                meta_action = avg_q.argmax().item()
        
        self.last_action = meta_action
        return self.last_decisions[meta_action]

    def update(self, arm, reward):
        # í•˜ìœ„ ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸
        for agent in self.sub_agents:
            if hasattr(agent, 'update'): agent.update(arm, reward)
        
        # History ì—…ë°ì´íŠ¸
        self.history.append((arm, reward))

# =========================================================
# [Part 3] í™˜ê²½ ë° ë©”ì¸ í‰ê°€ (ì†ë„ ê°œì„ ë¨)
# =========================================================
class NonStationaryEnvironment:
    def __init__(self, json_path):
        with open(json_path, 'r', encoding='utf-8') as f:
            self.levels = json.load(f)
        self.levels.sort(key=lambda x: x['start_trial'])
        if self.levels: self.max_trial = self.levels[-1]['end_trial'] + 1
        else: self.max_trial = 2000

    def get_reward_probabilities(self, t):
        for level_data in self.levels:
            if level_data['start_trial'] <= t <= level_data['end_trial']:
                return [level_data['p0'], level_data['p1']]
        return [0.5, 0.5]

    def get_reward(self, arm, t):
        probs = self.get_reward_probabilities(t)
        if arm < len(probs): return 1 if random.random() < probs[arm] else 0
        return 0

def run_evaluation():
    json_files = ['rwd_seq_example_01.json', 'rwd_seq_example_02.json', 'rwd_seq_example_03.json']
    N_EPISODES = 100
    
    print(f"\nğŸš€ Evaluating 3-Fold IQN Ensemble (100 Episodes each)...")
    
    # [ìˆ˜ì •] ì—ì´ì „íŠ¸ë¥¼ ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ìƒì„± (ëª¨ë¸ ë¡œë”© 1íšŒ)
    agent = IQN3FoldEnsembleAgent()
    if not agent.models:
        print("âŒ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. train_3fold_iqn.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    print(f"{'File Name':<25} | {'Avg Score':<10} | {'Max':<6} | {'Min':<6}")
    print("-" * 60)
    
    for fname in json_files:
        if not os.path.exists(fname):
            print(f"{fname:<25} | FILE NOT FOUND")
            continue
            
        scores = []
        for ep in range(N_EPISODES):
            env = NonStationaryEnvironment(fname)
            
            # [ìˆ˜ì •] ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì§€ ì•Šê³  ë©”ëª¨ë¦¬ë§Œ ì´ˆê¸°í™”
            agent.reset_episode() 
            
            total_r = 0
            for t in range(env.max_trial):
                action = agent.select_arm()
                reward = env.get_reward(action, t)
                agent.update(action, reward)
                total_r += reward
            scores.append(total_r)
            
        avg = sum(scores) / len(scores)
        print(f"{fname:<25} | {avg:<10.1f} | {max(scores):<6} | {min(scores):<6}")

    print("-" * 60)
    print("âœ… Evaluation Complete!")

if __name__ == "__main__":
    run_evaluation()
